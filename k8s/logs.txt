
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                  ARGS                  ‚îÇ PROFILE  ‚îÇ     USER      ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=8192 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 14 Nov 25 12:29 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=8192 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:18 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=4096 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:23 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=2 --memory=4096 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:27 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=2048 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:36 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=2 --memory=2048 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:36 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=1024 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:36 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=4096 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:37 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=4 --memory=4096 ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 18 Nov 25 17:37 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 16:38 IST ‚îÇ 19 Dec 25 16:42 IST ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                         ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 17:02 IST ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                         ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 17:09 IST ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --memory=4096          ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 17:20 IST ‚îÇ 19 Dec 25 17:20 IST ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                         ‚îÇ minikube ‚îÇ administrator ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 17:21 IST ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/12/19 17:20:39
Running on machine: VLP098
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1219 17:20:39.634440   19205 out.go:360] Setting OutFile to fd 1 ...
I1219 17:20:39.634508   19205 out.go:413] isatty.IsTerminal(1) = true
I1219 17:20:39.634511   19205 out.go:374] Setting ErrFile to fd 2...
I1219 17:20:39.634514   19205 out.go:413] isatty.IsTerminal(2) = true
I1219 17:20:39.634647   19205 root.go:338] Updating PATH: /home/administrator/.minikube/bin
W1219 17:20:39.634728   19205 root.go:314] Error reading config file at /home/administrator/.minikube/config/config.json: open /home/administrator/.minikube/config/config.json: no such file or directory
I1219 17:20:39.635189   19205 out.go:368] Setting JSON to false
I1219 17:20:39.636167   19205 start.go:130] hostinfo: {"hostname":"VLP098","uptime":2021,"bootTime":1766143018,"procs":287,"os":"linux","platform":"linuxmint","platformFamily":"debian","platformVersion":"22.2","kernelVersion":"6.14.0-35-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"4764f608-e416-4a0b-aa59-1419079f1006"}
I1219 17:20:39.636232   19205 start.go:140] virtualization: kvm host
I1219 17:20:39.638227   19205 out.go:179] üòÑ  minikube v1.37.0 on Linuxmint 22.2
I1219 17:20:39.641778   19205 notify.go:220] Checking for updates...
I1219 17:20:39.642051   19205 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1219 17:20:39.643457   19205 driver.go:421] Setting default libvirt URI to qemu:///system
I1219 17:20:39.664895   19205 docker.go:123] docker version: linux-28.4.0:
I1219 17:20:39.664958   19205 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1219 17:20:40.591392   19205 info.go:266] docker info: {ID:8cc96f24-0be4-4534-8f91-73e7b4971dcf Containers:10 ContainersRunning:1 ContainersPaused:0 ContainersStopped:9 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:48 SystemTime:2025-12-19 17:20:40.582655186 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu Core 24 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8112902144 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:VLP098 Labels:[] ExperimentalBuild:false ServerVersion:28.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/home/administrator/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-ai] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/home/administrator/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-buildx /usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:/home/administrator/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-compose /usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:/home/administrator/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:/home/administrator/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-desktop] ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/home/administrator/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/home/administrator/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/home/administrator/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-mcp] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:offload Path:/home/administrator/.docker/cli-plugins/docker-offload SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-offload] ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.1] map[Name:sandbox Path:/home/administrator/.docker/cli-plugins/docker-sandbox SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-sandbox] ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.3.1] map[Name:sbom Path:/home/administrator/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/home/administrator/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1219 17:20:40.591513   19205 docker.go:318] overlay module found
I1219 17:20:40.593320   19205 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1219 17:20:40.594903   19205 start.go:304] selected driver: docker
I1219 17:20:40.594910   19205 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1219 17:20:40.594970   19205 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1219 17:20:40.595699   19205 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1219 17:20:40.693612   19205 info.go:266] docker info: {ID:8cc96f24-0be4-4534-8f91-73e7b4971dcf Containers:10 ContainersRunning:1 ContainersPaused:0 ContainersStopped:9 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:48 SystemTime:2025-12-19 17:20:40.685901062 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu Core 24 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8112902144 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:VLP098 Labels:[] ExperimentalBuild:false ServerVersion:28.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/home/administrator/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-ai] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/home/administrator/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-buildx /usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:/home/administrator/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-compose /usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:/home/administrator/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:/home/administrator/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-desktop] ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/home/administrator/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/home/administrator/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/home/administrator/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-mcp] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:offload Path:/home/administrator/.docker/cli-plugins/docker-offload SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-offload] ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.1] map[Name:sandbox Path:/home/administrator/.docker/cli-plugins/docker-sandbox SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-sandbox] ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.3.1] map[Name:sbom Path:/home/administrator/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/home/administrator/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
W1219 17:20:40.693818   19205 out.go:285] ‚ùó  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I1219 17:20:40.693851   19205 cni.go:84] Creating CNI manager for ""
I1219 17:20:40.694445   19205 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1219 17:20:40.694489   19205 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1219 17:20:40.697517   19205 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1219 17:20:40.699270   19205 cache.go:123] Beginning downloading kic base image for docker with docker
I1219 17:20:40.700941   19205 out.go:179] üöú  Pulling base image v0.0.48 ...
I1219 17:20:40.702538   19205 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1219 17:20:40.702584   19205 preload.go:146] Found local preload: /home/administrator/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1219 17:20:40.702589   19205 cache.go:58] Caching tarball of preloaded images
I1219 17:20:40.702707   19205 preload.go:172] Found /home/administrator/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1219 17:20:40.702718   19205 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1219 17:20:40.702801   19205 profile.go:143] Saving config to /home/administrator/.minikube/profiles/minikube/config.json ...
I1219 17:20:40.702952   19205 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1219 17:20:40.722524   19205 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1219 17:20:40.722531   19205 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1219 17:20:40.722541   19205 cache.go:232] Successfully downloaded all kic artifacts
I1219 17:20:40.722557   19205 start.go:360] acquireMachinesLock for minikube: {Name:mka7fe76a2fe1247be1f6616557b5d842178cf84 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1219 17:20:40.722632   19205 start.go:364] duration metric: took 63.752¬µs to acquireMachinesLock for "minikube"
I1219 17:20:40.722642   19205 start.go:96] Skipping create...Using existing machine configuration
I1219 17:20:40.722645   19205 fix.go:54] fixHost starting: 
I1219 17:20:40.722793   19205 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1219 17:20:40.737833   19205 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1219 17:20:40.737851   19205 fix.go:138] unexpected machine state, will restart: <nil>
I1219 17:20:40.739746   19205 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I1219 17:20:40.739808   19205 cli_runner.go:164] Run: docker start minikube
I1219 17:20:40.960385   19205 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1219 17:20:40.974372   19205 kic.go:430] container "minikube" state is running.
I1219 17:20:40.974678   19205 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1219 17:20:40.987027   19205 profile.go:143] Saving config to /home/administrator/.minikube/profiles/minikube/config.json ...
I1219 17:20:40.987196   19205 machine.go:93] provisionDockerMachine start ...
I1219 17:20:40.987245   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:41.000668   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:41.001026   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:41.001031   19205 main.go:141] libmachine: About to run SSH command:
hostname
I1219 17:20:41.001571   19205 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:36480->127.0.0.1:32768: read: connection reset by peer
I1219 17:20:44.138134   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1219 17:20:44.138150   19205 ubuntu.go:182] provisioning hostname "minikube"
I1219 17:20:44.138196   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:44.153331   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:44.153526   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:44.153531   19205 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1219 17:20:44.312260   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1219 17:20:44.312316   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:44.325215   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:44.325472   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:44.325487   19205 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1219 17:20:44.455940   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1219 17:20:44.455955   19205 ubuntu.go:188] set auth options {CertDir:/home/administrator/.minikube CaCertPath:/home/administrator/.minikube/certs/ca.pem CaPrivateKeyPath:/home/administrator/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/administrator/.minikube/machines/server.pem ServerKeyPath:/home/administrator/.minikube/machines/server-key.pem ClientKeyPath:/home/administrator/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/administrator/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/administrator/.minikube}
I1219 17:20:44.455987   19205 ubuntu.go:190] setting up certificates
I1219 17:20:44.455995   19205 provision.go:84] configureAuth start
I1219 17:20:44.456058   19205 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1219 17:20:44.473559   19205 provision.go:143] copyHostCerts
I1219 17:20:44.477216   19205 exec_runner.go:144] found /home/administrator/.minikube/key.pem, removing ...
I1219 17:20:44.477225   19205 exec_runner.go:203] rm: /home/administrator/.minikube/key.pem
I1219 17:20:44.477261   19205 exec_runner.go:151] cp: /home/administrator/.minikube/certs/key.pem --> /home/administrator/.minikube/key.pem (1675 bytes)
I1219 17:20:44.477502   19205 exec_runner.go:144] found /home/administrator/.minikube/ca.pem, removing ...
I1219 17:20:44.477506   19205 exec_runner.go:203] rm: /home/administrator/.minikube/ca.pem
I1219 17:20:44.477523   19205 exec_runner.go:151] cp: /home/administrator/.minikube/certs/ca.pem --> /home/administrator/.minikube/ca.pem (1094 bytes)
I1219 17:20:44.477735   19205 exec_runner.go:144] found /home/administrator/.minikube/cert.pem, removing ...
I1219 17:20:44.477738   19205 exec_runner.go:203] rm: /home/administrator/.minikube/cert.pem
I1219 17:20:44.477755   19205 exec_runner.go:151] cp: /home/administrator/.minikube/certs/cert.pem --> /home/administrator/.minikube/cert.pem (1139 bytes)
I1219 17:20:44.477889   19205 provision.go:117] generating server cert: /home/administrator/.minikube/machines/server.pem ca-key=/home/administrator/.minikube/certs/ca.pem private-key=/home/administrator/.minikube/certs/ca-key.pem org=administrator.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1219 17:20:44.697996   19205 provision.go:177] copyRemoteCerts
I1219 17:20:44.698030   19205 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1219 17:20:44.698054   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:44.711279   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:44.803591   19205 ssh_runner.go:362] scp /home/administrator/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1219 17:20:44.829292   19205 ssh_runner.go:362] scp /home/administrator/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I1219 17:20:44.853137   19205 ssh_runner.go:362] scp /home/administrator/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1219 17:20:44.880056   19205 provision.go:87] duration metric: took 424.052417ms to configureAuth
I1219 17:20:44.880070   19205 ubuntu.go:206] setting minikube options for container-runtime
I1219 17:20:44.880181   19205 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1219 17:20:44.880212   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:44.893045   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:44.893212   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:44.893216   19205 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1219 17:20:45.023437   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1219 17:20:45.023457   19205 ubuntu.go:71] root file system type: overlay
I1219 17:20:45.023610   19205 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1219 17:20:45.023676   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.036127   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:45.036408   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:45.036484   19205 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1219 17:20:45.177613   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1219 17:20:45.177661   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.195041   19205 main.go:141] libmachine: Using SSH client type: native
I1219 17:20:45.195207   19205 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1219 17:20:45.195218   19205 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1219 17:20:45.330044   19205 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1219 17:20:45.330058   19205 machine.go:96] duration metric: took 4.342856009s to provisionDockerMachine
I1219 17:20:45.330081   19205 start.go:293] postStartSetup for "minikube" (driver="docker")
I1219 17:20:45.330091   19205 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1219 17:20:45.330138   19205 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1219 17:20:45.330173   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.342644   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:45.438770   19205 ssh_runner.go:195] Run: cat /etc/os-release
I1219 17:20:45.441590   19205 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1219 17:20:45.441606   19205 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1219 17:20:45.441613   19205 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1219 17:20:45.441618   19205 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1219 17:20:45.441630   19205 filesync.go:126] Scanning /home/administrator/.minikube/addons for local assets ...
I1219 17:20:45.441840   19205 filesync.go:126] Scanning /home/administrator/.minikube/files for local assets ...
I1219 17:20:45.441935   19205 start.go:296] duration metric: took 111.849019ms for postStartSetup
I1219 17:20:45.441966   19205 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1219 17:20:45.441987   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.455278   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:45.549207   19205 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1219 17:20:45.553267   19205 fix.go:56] duration metric: took 4.830616512s for fixHost
I1219 17:20:45.553279   19205 start.go:83] releasing machines lock for "minikube", held for 4.830642823s
I1219 17:20:45.553331   19205 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1219 17:20:45.568453   19205 ssh_runner.go:195] Run: cat /version.json
I1219 17:20:45.568486   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.568542   19205 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1219 17:20:45.568604   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:45.583747   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:45.584853   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:46.922955   19205 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.35439756s)
W1219 17:20:46.922992   19205 start.go:868] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Failed to connect to registry.k8s.io port 443 after 1213 ms: Connection timed out
I1219 17:20:46.923033   19205 ssh_runner.go:235] Completed: cat /version.json: (1.354568297s)
I1219 17:20:46.923105   19205 ssh_runner.go:195] Run: systemctl --version
I1219 17:20:46.931035   19205 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1219 17:20:46.936004   19205 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1219 17:20:46.957668   19205 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1219 17:20:46.957713   19205 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1219 17:20:46.966315   19205 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1219 17:20:46.966333   19205 start.go:495] detecting cgroup driver to use...
I1219 17:20:46.966366   19205 detect.go:190] detected "systemd" cgroup driver on host os
I1219 17:20:46.966720   19205 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1219 17:20:46.985362   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1219 17:20:46.996203   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1219 17:20:47.004974   19205 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1219 17:20:47.005018   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1219 17:20:47.015714   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1219 17:20:47.026569   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1219 17:20:47.036385   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1219 17:20:47.046642   19205 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1219 17:20:47.056054   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1219 17:20:47.066490   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1219 17:20:47.083410   19205 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1219 17:20:47.114714   19205 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
W1219 17:20:47.132119   19205 out.go:285] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1219 17:20:47.132173   19205 out.go:285] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1219 17:20:47.141683   19205 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1219 17:20:47.158236   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:47.242032   19205 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1219 17:20:47.326010   19205 start.go:495] detecting cgroup driver to use...
I1219 17:20:47.326043   19205 detect.go:190] detected "systemd" cgroup driver on host os
I1219 17:20:47.326078   19205 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1219 17:20:47.336894   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1219 17:20:47.346386   19205 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1219 17:20:47.360754   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1219 17:20:47.373138   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1219 17:20:47.387339   19205 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1219 17:20:47.401613   19205 ssh_runner.go:195] Run: which cri-dockerd
I1219 17:20:47.404452   19205 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1219 17:20:47.412217   19205 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1219 17:20:47.431446   19205 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1219 17:20:47.485867   19205 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1219 17:20:47.545111   19205 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1219 17:20:47.545183   19205 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1219 17:20:47.561641   19205 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1219 17:20:47.571629   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:47.628015   19205 ssh_runner.go:195] Run: sudo systemctl restart docker
I1219 17:20:48.411207   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1219 17:20:48.420685   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1219 17:20:48.430630   19205 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1219 17:20:48.441096   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1219 17:20:48.450749   19205 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1219 17:20:48.507089   19205 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1219 17:20:48.564966   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:48.620983   19205 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1219 17:20:48.647661   19205 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1219 17:20:48.657033   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:48.713529   19205 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1219 17:20:48.992234   19205 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1219 17:20:49.002225   19205 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1219 17:20:49.002265   19205 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1219 17:20:49.005341   19205 start.go:563] Will wait 60s for crictl version
I1219 17:20:49.005375   19205 ssh_runner.go:195] Run: which crictl
I1219 17:20:49.007979   19205 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1219 17:20:49.140127   19205 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1219 17:20:49.140178   19205 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1219 17:20:49.261803   19205 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1219 17:20:49.284028   19205 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1219 17:20:49.284097   19205 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1219 17:20:49.299706   19205 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1219 17:20:49.304281   19205 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1219 17:20:49.316927   19205 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1219 17:20:49.316996   19205 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1219 17:20:49.317022   19205 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1219 17:20:49.333888   19205 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1219 17:20:49.333899   19205 docker.go:621] Images already preloaded, skipping extraction
I1219 17:20:49.333958   19205 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1219 17:20:49.351857   19205 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1219 17:20:49.351878   19205 cache_images.go:85] Images are preloaded, skipping loading
I1219 17:20:49.351884   19205 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1219 17:20:49.352013   19205 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1219 17:20:49.352063   19205 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1219 17:20:49.598246   19205 cni.go:84] Creating CNI manager for ""
I1219 17:20:49.598260   19205 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1219 17:20:49.598278   19205 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1219 17:20:49.598292   19205 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1219 17:20:49.598387   19205 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1219 17:20:49.598438   19205 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1219 17:20:49.608239   19205 binaries.go:44] Found k8s binaries, skipping transfer
I1219 17:20:49.608281   19205 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1219 17:20:49.616278   19205 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1219 17:20:49.633475   19205 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1219 17:20:49.648885   19205 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1219 17:20:49.664693   19205 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1219 17:20:49.667569   19205 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1219 17:20:49.676821   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:49.730977   19205 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1219 17:20:49.754440   19205 certs.go:68] Setting up /home/administrator/.minikube/profiles/minikube for IP: 192.168.49.2
I1219 17:20:49.754448   19205 certs.go:194] generating shared ca certs ...
I1219 17:20:49.754459   19205 certs.go:226] acquiring lock for ca certs: {Name:mk9caefc43b9a928db4f05a40051a4f35d78e74f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1219 17:20:49.754612   19205 certs.go:235] skipping valid "minikubeCA" ca cert: /home/administrator/.minikube/ca.key
I1219 17:20:49.756546   19205 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/administrator/.minikube/proxy-client-ca.key
I1219 17:20:49.756551   19205 certs.go:256] generating profile certs ...
I1219 17:20:49.756610   19205 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/administrator/.minikube/profiles/minikube/client.key
I1219 17:20:49.756897   19205 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/administrator/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1219 17:20:49.757320   19205 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/administrator/.minikube/profiles/minikube/proxy-client.key
I1219 17:20:49.757456   19205 certs.go:484] found cert: /home/administrator/.minikube/certs/ca-key.pem (1675 bytes)
I1219 17:20:49.757484   19205 certs.go:484] found cert: /home/administrator/.minikube/certs/ca.pem (1094 bytes)
I1219 17:20:49.757511   19205 certs.go:484] found cert: /home/administrator/.minikube/certs/cert.pem (1139 bytes)
I1219 17:20:49.757531   19205 certs.go:484] found cert: /home/administrator/.minikube/certs/key.pem (1675 bytes)
I1219 17:20:49.758414   19205 ssh_runner.go:362] scp /home/administrator/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1219 17:20:49.782857   19205 ssh_runner.go:362] scp /home/administrator/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1219 17:20:49.808367   19205 ssh_runner.go:362] scp /home/administrator/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1219 17:20:49.829920   19205 ssh_runner.go:362] scp /home/administrator/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1219 17:20:49.851545   19205 ssh_runner.go:362] scp /home/administrator/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1219 17:20:49.873210   19205 ssh_runner.go:362] scp /home/administrator/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1219 17:20:49.901028   19205 ssh_runner.go:362] scp /home/administrator/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1219 17:20:49.923739   19205 ssh_runner.go:362] scp /home/administrator/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1219 17:20:49.945300   19205 ssh_runner.go:362] scp /home/administrator/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1219 17:20:49.967243   19205 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1219 17:20:49.985871   19205 ssh_runner.go:195] Run: openssl version
I1219 17:20:49.995767   19205 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1219 17:20:50.007710   19205 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1219 17:20:50.011064   19205 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec 19 11:12 /usr/share/ca-certificates/minikubeCA.pem
I1219 17:20:50.011094   19205 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1219 17:20:50.017415   19205 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1219 17:20:50.026588   19205 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1219 17:20:50.029869   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1219 17:20:50.038179   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1219 17:20:50.047300   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1219 17:20:50.056140   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1219 17:20:50.062267   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1219 17:20:50.069322   19205 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1219 17:20:50.075234   19205 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1219 17:20:50.075333   19205 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1219 17:20:50.096905   19205 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1219 17:20:50.107095   19205 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1219 17:20:50.107102   19205 kubeadm.go:589] restartPrimaryControlPlane start ...
I1219 17:20:50.107134   19205 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1219 17:20:50.117055   19205 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1219 17:20:50.117436   19205 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I1219 17:20:50.118510   19205 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1219 17:20:50.127300   19205 kubeadm.go:626] The running cluster does not require reconfiguration: 192.168.49.2
I1219 17:20:50.127317   19205 kubeadm.go:593] duration metric: took 20.211609ms to restartPrimaryControlPlane
I1219 17:20:50.127324   19205 kubeadm.go:394] duration metric: took 52.096702ms to StartCluster
I1219 17:20:50.127335   19205 settings.go:142] acquiring lock: {Name:mk590b574153e5887c731a6b0bf35ea8ea0421f2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1219 17:20:50.127409   19205 settings.go:150] Updating kubeconfig:  /home/administrator/.kube/config
I1219 17:20:50.127797   19205 lock.go:35] WriteFile acquiring /home/administrator/.kube/config: {Name:mkdbc11c92025c1fa57f8c1ee551c237041c5d15 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1219 17:20:50.127975   19205 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1219 17:20:50.128044   19205 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1219 17:20:50.128113   19205 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1219 17:20:50.128119   19205 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1219 17:20:50.128127   19205 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1219 17:20:50.128131   19205 addons.go:247] addon storage-provisioner should already be in state true
I1219 17:20:50.128145   19205 host.go:66] Checking if "minikube" exists ...
I1219 17:20:50.128144   19205 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1219 17:20:50.128159   19205 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1219 17:20:50.128336   19205 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1219 17:20:50.128409   19205 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1219 17:20:50.130314   19205 out.go:179] üîé  Verifying Kubernetes components...
I1219 17:20:50.133983   19205 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1219 17:20:50.147701   19205 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1219 17:20:50.147708   19205 addons.go:247] addon default-storageclass should already be in state true
I1219 17:20:50.147724   19205 host.go:66] Checking if "minikube" exists ...
I1219 17:20:50.147948   19205 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1219 17:20:50.147949   19205 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1219 17:20:50.149913   19205 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1219 17:20:50.149922   19205 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1219 17:20:50.149961   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:50.166207   19205 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1219 17:20:50.166219   19205 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1219 17:20:50.166272   19205 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1219 17:20:50.169310   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:50.187980   19205 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrator/.minikube/machines/minikube/id_rsa Username:docker}
I1219 17:20:50.219777   19205 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1219 17:20:50.238553   19205 api_server.go:52] waiting for apiserver process to appear ...
I1219 17:20:50.238605   19205 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1219 17:20:50.281119   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1219 17:20:50.289971   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1219 17:20:50.490467   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.490497   19205 retry.go:31] will retry after 168.814337ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1219 17:20:50.490511   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.490519   19205 retry.go:31] will retry after 243.117811ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.659736   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1219 17:20:50.706844   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.706872   19205 retry.go:31] will retry after 289.368197ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.734058   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1219 17:20:50.738995   19205 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1219 17:20:50.808232   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.808247   19205 retry.go:31] will retry after 210.16332ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:50.996457   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1219 17:20:51.019281   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1219 17:20:51.060179   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:51.060195   19205 retry.go:31] will retry after 672.632086ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1219 17:20:51.078917   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:51.078934   19205 retry.go:31] will retry after 478.640286ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:51.239155   19205 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1219 17:20:51.248834   19205 api_server.go:72] duration metric: took 1.120837174s to wait for apiserver process to appear ...
I1219 17:20:51.248846   19205 api_server.go:88] waiting for apiserver healthz status ...
I1219 17:20:51.248864   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:51.249101   19205 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1219 17:20:51.558545   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1219 17:20:51.609901   19205 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:51.609920   19205 retry.go:31] will retry after 587.524894ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1219 17:20:51.733098   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1219 17:20:51.749187   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:52.198139   19205 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1219 17:20:53.210655   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1219 17:20:53.210707   19205 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1219 17:20:53.210721   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:53.216161   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1219 17:20:53.216174   19205 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1219 17:20:53.249380   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:53.260061   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1219 17:20:53.260085   19205 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1219 17:20:53.665000   19205 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.931881349s)
I1219 17:20:53.665040   19205 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.466888014s)
I1219 17:20:53.676376   19205 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I1219 17:20:53.678445   19205 addons.go:514] duration metric: took 3.550394182s for enable addons: enabled=[storage-provisioner default-storageclass]
I1219 17:20:53.749608   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:53.752856   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1219 17:20:53.752866   19205 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1219 17:20:54.249487   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:54.252544   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1219 17:20:54.252554   19205 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1219 17:20:54.749251   19205 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1219 17:20:54.752339   19205 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1219 17:20:54.753176   19205 api_server.go:141] control plane version: v1.34.0
I1219 17:20:54.753188   19205 api_server.go:131] duration metric: took 3.504337919s to wait for apiserver health ...
I1219 17:20:54.753194   19205 system_pods.go:43] waiting for kube-system pods to appear ...
I1219 17:20:54.774016   19205 system_pods.go:59] 7 kube-system pods found
I1219 17:20:54.774034   19205 system_pods.go:61] "coredns-66bc5c9577-95nxk" [ceb771cc-2826-4f74-9956-19b98d4080e6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1219 17:20:54.774039   19205 system_pods.go:61] "etcd-minikube" [818ff49b-6807-4dd1-8431-4912bcb1f154] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1219 17:20:54.774044   19205 system_pods.go:61] "kube-apiserver-minikube" [ba2601b7-c0e2-4912-a107-f53491c19972] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1219 17:20:54.774049   19205 system_pods.go:61] "kube-controller-manager-minikube" [a448526e-03cd-4e35-9a96-496ef3062022] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1219 17:20:54.774054   19205 system_pods.go:61] "kube-proxy-vxdn4" [aa3c9404-e76f-4baa-b84e-11f308cab1e6] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1219 17:20:54.774056   19205 system_pods.go:61] "kube-scheduler-minikube" [f180db06-43e3-4314-aa8c-2226756e10ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1219 17:20:54.774059   19205 system_pods.go:61] "storage-provisioner" [8738b2d9-6afb-4982-abdb-50fac024afc3] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1219 17:20:54.774062   19205 system_pods.go:74] duration metric: took 20.864904ms to wait for pod list to return data ...
I1219 17:20:54.774069   19205 kubeadm.go:578] duration metric: took 4.646079416s to wait for: map[apiserver:true system_pods:true]
I1219 17:20:54.774078   19205 node_conditions.go:102] verifying NodePressure condition ...
I1219 17:20:54.777057   19205 node_conditions.go:122] node storage ephemeral capacity is 490617784Ki
I1219 17:20:54.777072   19205 node_conditions.go:123] node cpu capacity is 8
I1219 17:20:54.777079   19205 node_conditions.go:105] duration metric: took 2.998599ms to run NodePressure ...
I1219 17:20:54.777087   19205 start.go:241] waiting for startup goroutines ...
I1219 17:20:54.777091   19205 start.go:246] waiting for cluster config update ...
I1219 17:20:54.777098   19205 start.go:255] writing updated cluster config ...
I1219 17:20:54.777280   19205 ssh_runner.go:195] Run: rm -f paused
I1219 17:20:54.824454   19205 start.go:617] kubectl: 1.35.0, cluster: 1.34.0 (minor skew: 1)
I1219 17:20:54.833059   19205 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 19 11:50:48 minikube dockerd[816]: time="2025-12-19T11:50:48.405924229Z" level=info msg="Completed buildkit initialization"
Dec 19 11:50:48 minikube dockerd[816]: time="2025-12-19T11:50:48.409203685Z" level=info msg="Daemon has completed initialization"
Dec 19 11:50:48 minikube dockerd[816]: time="2025-12-19T11:50:48.409279856Z" level=info msg="API listen on /var/run/docker.sock"
Dec 19 11:50:48 minikube dockerd[816]: time="2025-12-19T11:50:48.409356827Z" level=info msg="API listen on /run/docker.sock"
Dec 19 11:50:48 minikube dockerd[816]: time="2025-12-19T11:50:48.409392262Z" level=info msg="API listen on [::]:2376"
Dec 19 11:50:48 minikube systemd[1]: Started Docker Application Container Engine.
Dec 19 11:50:48 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Start docker client with request timeout 0s"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Loaded network plugin cni"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Setting cgroupDriver systemd"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 19 11:50:48 minikube cri-dockerd[1130]: time="2025-12-19T11:50:48Z" level=info msg="Start cri-dockerd grpc backend"
Dec 19 11:50:48 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 19 11:50:50 minikube cri-dockerd[1130]: time="2025-12-19T11:50:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-95nxk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"648192abeb60c61b454e13b87ceea109bd7ca57a72a0171f5e954a25b0404b5a\""
Dec 19 11:50:50 minikube cri-dockerd[1130]: time="2025-12-19T11:50:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e809c56018ad0a2fc0733ff883bd0adfc00522225f1838d2b9c1d0260e46db8/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options ndots:0 edns0 trust-ad]"
Dec 19 11:50:50 minikube cri-dockerd[1130]: time="2025-12-19T11:50:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2d1f9b5994627d861e8dd88985ab056f4aa5d86095b87a7602428f05815b4e0d/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options edns0 trust-ad ndots:0]"
Dec 19 11:50:50 minikube cri-dockerd[1130]: time="2025-12-19T11:50:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0c40cea71b9823bd5fcd624df1ee737d065d5927e91e140a6c5e87afa9533d26/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options ndots:0 edns0 trust-ad]"
Dec 19 11:50:50 minikube cri-dockerd[1130]: time="2025-12-19T11:50:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e596b00afefa73b10e238443a05d9c8febfaef90df9df4a0e3b66f8dc2104b71/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options edns0 trust-ad ndots:0]"
Dec 19 11:50:53 minikube cri-dockerd[1130]: time="2025-12-19T11:50:53Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 19 11:50:54 minikube cri-dockerd[1130]: time="2025-12-19T11:50:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f2c224234749763ae87fce6f4eb41a57626e81d29c66528bc50fcbefd296354d/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options edns0 trust-ad ndots:0]"
Dec 19 11:50:54 minikube cri-dockerd[1130]: time="2025-12-19T11:50:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e2f0a7ad98a96bfe8e08c78be9484149563f27b62cd42762367280d2615890e/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options edns0 trust-ad ndots:0]"
Dec 19 11:50:54 minikube cri-dockerd[1130]: time="2025-12-19T11:50:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1efaa1baecc16a632e7aff3f33973fedca3a1ff3a213a0fb9c130fd229e9002f/resolv.conf as [nameserver 192.168.49.1 search digzplacements.com options edns0 trust-ad ndots:0]"
Dec 19 11:51:07 minikube cri-dockerd[1130]: time="2025-12-19T11:51:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a1c700523461454758d43a1bc8c7d4f0787fdb575d7e2a5348f3b7a198d23d5/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local digzplacements.com options ndots:5]"
Dec 19 11:51:07 minikube cri-dockerd[1130]: time="2025-12-19T11:51:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/921c72458b96d94164109148d25806dc917343017bb25670956a56fb75cb0244/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local digzplacements.com options ndots:5]"
Dec 19 11:51:22 minikube dockerd[816]: time="2025-12-19T11:51:22.798136535Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:51:22 minikube dockerd[816]: time="2025-12-19T11:51:22.798172560Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:51:22 minikube dockerd[816]: time="2025-12-19T11:51:22.803623529Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:51:24 minikube dockerd[816]: time="2025-12-19T11:51:24.954056985Z" level=info msg="ignoring event" container=9a2ca6412bf9ca250c1881066bb7d7fafbe8fb2e7838fe7c74ec58a547730aaf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 19 11:51:37 minikube dockerd[816]: time="2025-12-19T11:51:37.807860017Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:51:37 minikube dockerd[816]: time="2025-12-19T11:51:37.807923804Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:51:37 minikube dockerd[816]: time="2025-12-19T11:51:37.813518927Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:51:52 minikube dockerd[816]: time="2025-12-19T11:51:52.818621623Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:51:52 minikube dockerd[816]: time="2025-12-19T11:51:52.818665537Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:51:52 minikube dockerd[816]: time="2025-12-19T11:51:52.824251462Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:07 minikube dockerd[816]: time="2025-12-19T11:52:07.828930125Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:07 minikube dockerd[816]: time="2025-12-19T11:52:07.828961223Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:07 minikube dockerd[816]: time="2025-12-19T11:52:07.831044952Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:31 minikube dockerd[816]: time="2025-12-19T11:52:31.086219660Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:52:31 minikube dockerd[816]: time="2025-12-19T11:52:31.086254403Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:52:31 minikube dockerd[816]: time="2025-12-19T11:52:31.091833679Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 19 11:52:46 minikube dockerd[816]: time="2025-12-19T11:52:46.096956631Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:46 minikube dockerd[816]: time="2025-12-19T11:52:46.096991675Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:52:46 minikube dockerd[816]: time="2025-12-19T11:52:46.102585358Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:27 minikube dockerd[816]: time="2025-12-19T11:53:27.090253125Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:27 minikube dockerd[816]: time="2025-12-19T11:53:27.090283047Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:27 minikube dockerd[816]: time="2025-12-19T11:53:27.092316093Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:53 minikube dockerd[816]: time="2025-12-19T11:53:53.087495606Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:53 minikube dockerd[816]: time="2025-12-19T11:53:53.087595195Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:53:53 minikube dockerd[816]: time="2025-12-19T11:53:53.096117800Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:08 minikube dockerd[816]: time="2025-12-19T11:55:08.086549781Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:08 minikube dockerd[816]: time="2025-12-19T11:55:08.086577313Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:08 minikube dockerd[816]: time="2025-12-19T11:55:08.092220478Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:37 minikube dockerd[816]: time="2025-12-19T11:55:37.112648038Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:37 minikube dockerd[816]: time="2025-12-19T11:55:37.112705646Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"
Dec 19 11:55:37 minikube dockerd[816]: time="2025-12-19T11:55:37.118300326Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": context deadline exceeded"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
0ef6fde88ba77       6e38f40d628db       6 minutes ago       Running             storage-provisioner       3                   0e2f0a7ad98a9       storage-provisioner
ff49b6bed53b4       df0860106674d       6 minutes ago       Running             kube-proxy                1                   1efaa1baecc16       kube-proxy-vxdn4
9a2ca6412bf9c       6e38f40d628db       6 minutes ago       Exited              storage-provisioner       2                   0e2f0a7ad98a9       storage-provisioner
0dd8db60de0da       52546a367cc9e       6 minutes ago       Running             coredns                   1                   f2c2242347497       coredns-66bc5c9577-95nxk
9f901e394d307       a0af72f2ec6d6       7 minutes ago       Running             kube-controller-manager   1                   0c40cea71b982       kube-controller-manager-minikube
c4360ce59f201       90550c43ad2bc       7 minutes ago       Running             kube-apiserver            1                   e596b00afefa7       kube-apiserver-minikube
8881521ce9485       46169d968e920       7 minutes ago       Running             kube-scheduler            1                   2d1f9b5994627       kube-scheduler-minikube
ca4ab9aaf1d88       5f1f5298c888d       7 minutes ago       Running             etcd                      1                   8e809c56018ad       etcd-minikube
82685d472607e       52546a367cc9e       45 minutes ago      Exited              coredns                   0                   648192abeb60c       coredns-66bc5c9577-95nxk
aea30cca2693e       df0860106674d       45 minutes ago      Exited              kube-proxy                0                   34183688b5ad8       kube-proxy-vxdn4
fefedb8440ab5       5f1f5298c888d       45 minutes ago      Exited              etcd                      0                   59b93138a27fd       etcd-minikube
2ba52e81e8782       a0af72f2ec6d6       45 minutes ago      Exited              kube-controller-manager   0                   b2d882e421229       kube-controller-manager-minikube
82299792d87c7       90550c43ad2bc       45 minutes ago      Exited              kube-apiserver            0                   364b5b28cfaf1       kube-apiserver-minikube
45a044acba4f9       46169d968e920       45 minutes ago      Exited              kube-scheduler            0                   c10b4ad118931       kube-scheduler-minikube


==> coredns [0dd8db60de0d] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:33584 - 14881 "HINFO IN 8084669751802030442.7385888642461624798. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.020891583s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [82685d472607] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:42417 - 5872 "HINFO IN 7631316338338818296.6300365921599458929. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.317743888s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.082576006s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_12_19T16_42_39_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Dec 2025 11:12:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 19 Dec 2025 11:57:52 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 19 Dec 2025 11:54:17 +0000   Fri, 19 Dec 2025 11:12:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 19 Dec 2025 11:54:17 +0000   Fri, 19 Dec 2025 11:12:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 19 Dec 2025 11:54:17 +0000   Fri, 19 Dec 2025 11:12:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 19 Dec 2025 11:54:17 +0000   Fri, 19 Dec 2025 11:12:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7922756Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7922756Ki
  pods:               110
System Info:
  Machine ID:                 36c15d14180e4615a32612383a6b520e
  System UUID:                0624d4c7-07fa-4f05-99da-dc4178a2bca2
  Boot ID:                    2b41294d-f947-40e4-a0e0-a0c0f4574c53
  Kernel Version:             6.14.0-35-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-admission-create-rmwd6        0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m45s
  ingress-nginx               ingress-nginx-admission-patch-fqhwk         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m45s
  ingress-nginx               ingress-nginx-controller-9cc49f96f-ch76m    100m (1%)     0 (0%)      90Mi (1%)        0 (0%)         6m45s
  kube-system                 coredns-66bc5c9577-95nxk                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     45m
  kube-system                 etcd-minikube                               100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         45m
  kube-system                 kube-apiserver-minikube                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         45m
  kube-system                 kube-controller-manager-minikube            200m (2%)     0 (0%)      0 (0%)           0 (0%)         45m
  kube-system                 kube-proxy-vxdn4                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m
  kube-system                 kube-scheduler-minikube                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         45m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             260Mi (3%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 45m                  kube-proxy       
  Normal   Starting                 6m57s                kube-proxy       
  Normal   Starting                 45m                  kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  45m                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  45m (x8 over 45m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    45m (x8 over 45m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     45m (x7 over 45m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    45m                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced  45m                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  45m                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID     45m                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                 45m                  kubelet          Starting kubelet.
  Normal   RegisteredNode           45m                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 7m2s                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  7m2s (x8 over 7m2s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    7m2s (x8 over 7m2s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     7m2s (x7 over 7m2s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  7m2s                 kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 6m59s                kubelet          Node minikube has been rebooted, boot id: 2b41294d-f947-40e4-a0e0-a0c0f4574c53
  Normal   RegisteredNode           6m56s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec19 11:16] [Firmware Bug]: TSC ADJUST: CPU0: -2916721823 force to 0
[  +0.000000] x86/cpu: SGX disabled or unsupported by BIOS.
[  -0.144553] [Firmware Bug]: TSC ADJUST differs within socket(s), fixing all errors
[  +0.158685]  #4 #5 #6 #7
[  +0.002152] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.457281] hpet_acpi_add: no address or irqs in _CRS
[  +0.015001] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.004481] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.072437] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.029303] integrity: Problem loading X.509 certificate -65
[  +0.571737] ata3.00: Model 'Micron 1100 SATA 512GB', rev ' M0DL003', applying quirks: zeroaftertrim
[Dec19 11:17] systemd-journald[351]: File /var/log/journal/4764f608e4164a0baa591419079f1006/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.349421] elan_i2c i2c-ELAN0633:00: supply vcc not found, using dummy regulator
[  +0.234584] spi-nor spi0.0: supply vcc not found, using dummy regulator
[  +1.264997] ACPI BIOS Error (bug): Could not resolve symbol [\_TZ.ETMD], AE_NOT_FOUND (20240827/psargs-332)

[  +0.000014] No Local Variables are initialized for Method [_OSC]

[  +0.000001] Initialized Arguments for Method [_OSC]:  (4 arguments defined for method invocation)
[  +0.000001]   Arg0:   000000006409c4e2 <Obj>           Buffer(16) 5D A8 3B B2 B7 C8 42 35
[  +0.000009]   Arg1:   000000004d29e368 <Obj>           Integer 0000000000000001
[  +0.000004]   Arg2:   000000003a59299d <Obj>           Integer 0000000000000002
[  +0.000003]   Arg3:   00000000aa597fd9 <Obj>           Buffer(8) 00 00 00 00 05 00 00 00

[  +0.000009] ACPI Error: Aborting method \_SB.IETM._OSC due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +5.737039] kauditd_printk_skb: 150 callbacks suppressed
[  +2.459249] systemd-journald[351]: File /var/log/journal/4764f608e4164a0baa591419079f1006/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[ +20.708534] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211


==> etcd [ca4ab9aaf1d8] <==
{"level":"info","ts":"2025-12-19T11:50:52.135345Z","caller":"embed/serve.go:283","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2025-12-19T11:50:52.519113Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55212","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.519258Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55196","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.536995Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55254","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.543142Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55274","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.552200Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55306","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.564672Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55320","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.571255Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55352","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.576722Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55366","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.582637Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55386","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.587879Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55406","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.593757Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55436","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.598673Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55438","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.604734Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55444","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.611431Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55456","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.615894Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55468","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.620967Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.626407Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55508","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.631047Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55534","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.637841Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55562","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.650724Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55582","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.657530Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55598","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.679041Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55624","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.684725Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55636","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.690217Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55672","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.696117Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.703130Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55688","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.709690Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55702","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.714691Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55728","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.725442Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55766","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.729816Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55776","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.735592Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55796","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.741609Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55818","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.746281Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.751252Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55866","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.757153Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55886","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.763479Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55914","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.767877Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55936","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.782558Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55980","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.787592Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55998","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.793127Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56020","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.798501Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56044","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.804375Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56060","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.816339Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56078","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.822445Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56108","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.827235Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56122","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.831755Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56140","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.836598Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56170","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.841386Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56178","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.847203Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56198","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.851992Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56212","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.857264Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56236","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.861900Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56260","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.867844Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56272","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.873695Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56290","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.878143Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56304","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.889264Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56336","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.893590Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56360","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.900310Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56384","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:50:52.931072Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56396","server-name":"","error":"EOF"}


==> etcd [fefedb8440ab] <==
{"level":"warn","ts":"2025-12-19T11:12:35.243350Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48104","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.255783Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48118","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.263621Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48138","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.268933Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48164","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.276037Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48188","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.280371Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.285724Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48230","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.291695Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.297044Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48272","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.301413Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48286","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.307198Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48304","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.313022Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48342","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.317676Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48358","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.322384Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48380","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.327413Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48402","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.332072Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48422","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.337617Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48442","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.342085Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48466","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.368912Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48480","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.374592Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48498","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.380278Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48526","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.385296Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48552","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.389836Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48570","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.394412Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.398952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48622","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.403929Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48626","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.408542Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48642","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.412941Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48662","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.417536Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.423020Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48700","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.427711Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48720","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.432344Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48738","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.437297Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48756","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.441778Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48784","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.446256Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48800","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.453300Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48810","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.458910Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48834","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.463655Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48854","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.469429Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.475074Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48902","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.479584Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48908","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.483827Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48924","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.489251Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48952","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.493884Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48968","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.501114Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.509107Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48986","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.514682Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49000","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.524650Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49030","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.529251Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49042","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.534062Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49056","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.538504Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49072","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.543250Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49086","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.547469Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49112","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.552006Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49130","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.564675Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49160","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.569962Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49174","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.574619Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49196","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:12:35.609389Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-19T11:15:49.280341Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"373.457525ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-12-19T11:15:50.281156Z","caller":"traceutil/trace.go:172","msg":"trace[276882354] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:553; }","duration":"1.357892401s","start":"2025-12-19T11:15:48.659981Z","end":"2025-12-19T11:15:50.017873Z","steps":["trace[276882354] 'agreement among raft nodes before linearized reading'  (duration: 81.923459ms)","trace[276882354] 'get authentication metadata'  (duration: 57.836564ms)","trace[276882354] 'range keys from bolt db'  (duration: 226.249922ms)"],"step_count":3}


==> kernel <==
 11:57:53 up 40 min,  0 users,  load average: 0.95, 1.01, 1.03
Linux minikube 6.14.0-35-generic #35~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct 14 13:55:17 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [82299792d87c] <==
I1219 11:12:35.892376       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1219 11:12:35.892433       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1219 11:12:35.892518       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1219 11:12:35.892668       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1219 11:12:35.892732       1 controller.go:142] Starting OpenAPI controller
I1219 11:12:35.892762       1 controller.go:90] Starting OpenAPI V3 controller
I1219 11:12:35.892772       1 repairip.go:210] Starting ipallocator-repair-controller
I1219 11:12:35.892780       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1219 11:12:35.892794       1 naming_controller.go:299] Starting NamingConditionController
I1219 11:12:35.892810       1 establishing_controller.go:81] Starting EstablishingController
I1219 11:12:35.892828       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1219 11:12:35.892839       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1219 11:12:35.892851       1 crd_finalizer.go:269] Starting CRDFinalizer
I1219 11:12:35.960214       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
E1219 11:12:35.965802       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1219 11:12:35.969926       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1219 11:12:35.969962       1 policy_source.go:240] refreshing policies
I1219 11:12:35.991160       1 cache.go:39] Caches are synced for LocalAvailability controller
I1219 11:12:35.991194       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1219 11:12:35.991198       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1219 11:12:35.991221       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1219 11:12:35.991231       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1219 11:12:35.991243       1 aggregator.go:171] initial CRD sync complete...
I1219 11:12:35.991260       1 autoregister_controller.go:144] Starting autoregister controller
I1219 11:12:35.991261       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1219 11:12:35.991265       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1219 11:12:35.991272       1 cache.go:39] Caches are synced for autoregister controller
I1219 11:12:35.991293       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1219 11:12:35.991692       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1219 11:12:35.991723       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1219 11:12:35.992122       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1219 11:12:35.992346       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1219 11:12:35.992930       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1219 11:12:35.994428       1 controller.go:667] quota admission added evaluator for: namespaces
I1219 11:12:35.999981       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1219 11:12:36.000110       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1219 11:12:36.007724       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1219 11:12:36.007998       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1219 11:12:36.168599       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1219 11:12:36.895889       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1219 11:12:36.898826       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1219 11:12:36.898837       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1219 11:12:37.331775       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1219 11:12:37.365768       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1219 11:12:37.500063       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1219 11:12:37.505016       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1219 11:12:37.505721       1 controller.go:667] quota admission added evaluator for: endpoints
I1219 11:12:37.509062       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1219 11:12:37.925120       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1219 11:12:38.624943       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1219 11:12:38.633854       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1219 11:12:38.639189       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1219 11:12:43.874062       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1219 11:12:43.977900       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1219 11:12:43.981310       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1219 11:12:44.023693       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1219 11:13:45.097423       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:13:55.740582       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:14:58.543119       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:15:02.081428       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [c4360ce59f20] <==
I1219 11:50:53.200240       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1219 11:50:53.200935       1 controller.go:142] Starting OpenAPI controller
I1219 11:50:53.200980       1 controller.go:90] Starting OpenAPI V3 controller
I1219 11:50:53.201001       1 naming_controller.go:299] Starting NamingConditionController
I1219 11:50:53.201015       1 establishing_controller.go:81] Starting EstablishingController
I1219 11:50:53.201033       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1219 11:50:53.201048       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1219 11:50:53.201059       1 crd_finalizer.go:269] Starting CRDFinalizer
I1219 11:50:53.201304       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1219 11:50:53.201326       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1219 11:50:53.201396       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1219 11:50:53.201417       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1219 11:50:53.201457       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1219 11:50:53.201484       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1219 11:50:53.208962       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1219 11:50:53.209522       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1219 11:50:53.264334       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1219 11:50:53.300003       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1219 11:50:53.300025       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1219 11:50:53.300105       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1219 11:50:53.300137       1 cache.go:39] Caches are synced for LocalAvailability controller
I1219 11:50:53.300199       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1219 11:50:53.300243       1 aggregator.go:171] initial CRD sync complete...
I1219 11:50:53.300251       1 autoregister_controller.go:144] Starting autoregister controller
I1219 11:50:53.300256       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1219 11:50:53.300261       1 cache.go:39] Caches are synced for autoregister controller
I1219 11:50:53.300609       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1219 11:50:53.300642       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1219 11:50:53.300650       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1219 11:50:53.302391       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1219 11:50:53.302408       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1219 11:50:53.302396       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
E1219 11:50:53.305634       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1219 11:50:53.306993       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1219 11:50:53.321477       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1219 11:50:53.329618       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1219 11:50:53.329638       1 policy_source.go:240] refreshing policies
I1219 11:50:53.338973       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1219 11:50:53.587088       1 controller.go:667] quota admission added evaluator for: endpoints
I1219 11:50:54.042255       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1219 11:50:54.202862       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1219 11:50:56.646261       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1219 11:50:56.993821       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1219 11:50:57.044904       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1219 11:51:07.096548       1 controller.go:667] quota admission added evaluator for: namespaces
I1219 11:51:07.121623       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1219 11:51:07.142275       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1219 11:51:07.193459       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.105.151.43"}
I1219 11:51:07.203976       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.99.95.151"}
I1219 11:51:07.220969       1 controller.go:667] quota admission added evaluator for: jobs.batch
I1219 11:51:54.334142       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:52:01.606047       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:53:02.741493       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:53:11.104069       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:54:11.119434       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:54:19.636666       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:55:12.593595       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:55:30.652822       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:56:36.023239       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1219 11:56:41.108980       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [2ba52e81e878] <==
I1219 11:12:42.671121       1 shared_informer.go:349] "Waiting for caches to sync" controller="taint"
I1219 11:12:42.822850       1 controllermanager.go:781] "Started controller" controller="volumeattributesclass-protection-controller"
I1219 11:12:42.822959       1 vac_protection_controller.go:206] "Starting VAC protection controller" logger="volumeattributesclass-protection-controller"
I1219 11:12:42.822970       1 shared_informer.go:349] "Waiting for caches to sync" controller="VAC protection"
I1219 11:12:42.828946       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1219 11:12:42.841695       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1219 11:12:42.843241       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1219 11:12:42.844273       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1219 11:12:42.847746       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1219 11:12:42.853321       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1219 11:12:42.857529       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1219 11:12:42.862861       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1219 11:12:42.869178       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1219 11:12:42.869193       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1219 11:12:42.869198       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1219 11:12:42.870994       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1219 11:12:42.871145       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1219 11:12:42.871241       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1219 11:12:42.871261       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1219 11:12:42.871376       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1219 11:12:42.871326       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1219 11:12:42.871430       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1219 11:12:42.872091       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1219 11:12:42.872221       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1219 11:12:42.872324       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1219 11:12:42.872323       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1219 11:12:42.872760       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1219 11:12:42.873162       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1219 11:12:42.873247       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1219 11:12:42.873380       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1219 11:12:42.873423       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1219 11:12:42.873486       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1219 11:12:42.873559       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1219 11:12:42.873652       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1219 11:12:42.874073       1 shared_informer.go:356] "Caches are synced" controller="job"
I1219 11:12:42.876773       1 shared_informer.go:356] "Caches are synced" controller="node"
I1219 11:12:42.876813       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1219 11:12:42.876842       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1219 11:12:42.876848       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1219 11:12:42.876853       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1219 11:12:42.880044       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1219 11:12:42.887383       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1219 11:12:42.922055       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1219 11:12:42.923127       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1219 11:12:42.923147       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1219 11:12:42.923160       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1219 11:12:42.923174       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1219 11:12:42.923189       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1219 11:12:42.923239       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1219 11:12:42.924329       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1219 11:12:42.924333       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1219 11:12:42.924342       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1219 11:12:42.925465       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1219 11:12:42.926863       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1219 11:12:42.929071       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1219 11:12:42.929091       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1219 11:12:42.930154       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1219 11:12:42.935425       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1219 11:12:42.938610       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1219 11:12:42.945095       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [9f901e394d30] <==
I1219 11:50:56.490821       1 shared_informer.go:349] "Waiting for caches to sync" controller="ephemeral"
I1219 11:50:56.543182       1 controllermanager.go:781] "Started controller" controller="resourceclaim-controller"
I1219 11:50:56.543204       1 controllermanager.go:733] "Controller is disabled by a feature gate" controller="device-taint-eviction-controller" requiredFeatureGates=["DynamicResourceAllocation","DRADeviceTaints"]
I1219 11:50:56.543233       1 controller.go:397] "Starting resource claim controller" logger="resourceclaim-controller"
I1219 11:50:56.543334       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource_claim"
I1219 11:50:56.549968       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1219 11:50:56.562616       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1219 11:50:56.566045       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1219 11:50:56.583522       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1219 11:50:56.591818       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1219 11:50:56.591895       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1219 11:50:56.591908       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1219 11:50:56.591900       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1219 11:50:56.591969       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1219 11:50:56.591971       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1219 11:50:56.591982       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1219 11:50:56.591989       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1219 11:50:56.592046       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1219 11:50:56.604418       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1219 11:50:56.610571       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1219 11:50:56.616800       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1219 11:50:56.617930       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1219 11:50:56.620154       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1219 11:50:56.631120       1 shared_informer.go:356] "Caches are synced" controller="node"
I1219 11:50:56.631172       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1219 11:50:56.631211       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1219 11:50:56.631218       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1219 11:50:56.631224       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1219 11:50:56.634426       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1219 11:50:56.637611       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1219 11:50:56.639909       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1219 11:50:56.640969       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1219 11:50:56.641010       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1219 11:50:56.641042       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1219 11:50:56.642145       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1219 11:50:56.642159       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1219 11:50:56.642165       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1219 11:50:56.642196       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1219 11:50:56.642243       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1219 11:50:56.642326       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1219 11:50:56.642378       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1219 11:50:56.642391       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1219 11:50:56.642397       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1219 11:50:56.642424       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1219 11:50:56.642372       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1219 11:50:56.642456       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1219 11:50:56.642750       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1219 11:50:56.642788       1 shared_informer.go:356] "Caches are synced" controller="job"
I1219 11:50:56.643734       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1219 11:50:56.643749       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1219 11:50:56.643741       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1219 11:50:56.645297       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1219 11:50:56.646395       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1219 11:50:56.646405       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1219 11:50:56.650120       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1219 11:50:56.657784       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1219 11:50:56.663023       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1219 11:50:56.663034       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1219 11:50:56.663043       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1219 11:50:56.666268       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [aea30cca2693] <==
I1219 11:12:44.910822       1 server_linux.go:53] "Using iptables proxy"
I1219 11:12:45.017899       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1219 11:12:45.118794       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1219 11:12:45.118827       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1219 11:12:45.118879       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1219 11:12:45.154849       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1219 11:12:45.154896       1 server_linux.go:132] "Using iptables Proxier"
I1219 11:12:45.159680       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1219 11:12:45.222134       1 server.go:527] "Version info" version="v1.34.0"
I1219 11:12:45.222151       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1219 11:12:45.223977       1 config.go:309] "Starting node config controller"
I1219 11:12:45.224001       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1219 11:12:45.224078       1 config.go:200] "Starting service config controller"
I1219 11:12:45.224098       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1219 11:12:45.224117       1 config.go:106] "Starting endpoint slice config controller"
I1219 11:12:45.224122       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1219 11:12:45.224133       1 config.go:403] "Starting serviceCIDR config controller"
I1219 11:12:45.224138       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1219 11:12:45.324325       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1219 11:12:45.324337       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1219 11:12:45.324335       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1219 11:12:45.324345       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [ff49b6bed53b] <==
I1219 11:50:55.072188       1 server_linux.go:53] "Using iptables proxy"
I1219 11:50:55.162954       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1219 11:50:55.263287       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1219 11:50:55.263312       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1219 11:50:55.263408       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1219 11:50:55.299237       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1219 11:50:55.299297       1 server_linux.go:132] "Using iptables Proxier"
I1219 11:50:55.306454       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1219 11:50:55.327690       1 server.go:527] "Version info" version="v1.34.0"
I1219 11:50:55.327787       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1219 11:50:55.330040       1 config.go:403] "Starting serviceCIDR config controller"
I1219 11:50:55.330062       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1219 11:50:55.330176       1 config.go:200] "Starting service config controller"
I1219 11:50:55.330184       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1219 11:50:55.330202       1 config.go:106] "Starting endpoint slice config controller"
I1219 11:50:55.330601       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1219 11:50:55.330320       1 config.go:309] "Starting node config controller"
I1219 11:50:55.330700       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1219 11:50:55.430837       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1219 11:50:55.430864       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1219 11:50:55.430864       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1219 11:50:55.430884       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [45a044acba4f] <==
I1219 11:12:34.638163       1 serving.go:386] Generated self-signed cert in-memory
W1219 11:12:35.910852       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1219 11:12:35.910899       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1219 11:12:35.910912       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1219 11:12:35.910921       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1219 11:12:35.928506       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1219 11:12:35.928529       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1219 11:12:35.931187       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1219 11:12:35.931214       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1219 11:12:35.931454       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1219 11:12:35.931498       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1219 11:12:35.933369       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1219 11:12:35.933445       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1219 11:12:35.933480       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1219 11:12:35.933542       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1219 11:12:35.933566       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1219 11:12:35.933582       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1219 11:12:35.933634       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1219 11:12:35.933653       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1219 11:12:35.935074       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1219 11:12:35.935087       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1219 11:12:35.935102       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1219 11:12:35.935074       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1219 11:12:35.935074       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1219 11:12:35.935153       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1219 11:12:35.935170       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1219 11:12:35.935177       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1219 11:12:35.935177       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1219 11:12:35.935192       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1219 11:12:35.935225       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1219 11:12:36.743322       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1219 11:12:36.747111       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1219 11:12:36.787320       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1219 11:12:36.809214       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1219 11:12:36.829636       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1219 11:12:36.911226       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1219 11:12:36.920907       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1219 11:12:36.949424       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1219 11:12:37.005220       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1219 11:12:37.073536       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1219 11:12:37.110339       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1219 11:12:37.149356       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1219 11:12:37.165295       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1219 11:12:37.174222       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1219 11:12:37.190052       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1219 11:12:37.215184       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I1219 11:12:40.231853       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [8881521ce948] <==
I1219 11:50:52.078534       1 serving.go:386] Generated self-signed cert in-memory
I1219 11:50:53.264482       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1219 11:50:53.264557       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1219 11:50:53.268907       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1219 11:50:53.268930       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1219 11:50:53.268946       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1219 11:50:53.268952       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1219 11:50:53.269209       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I1219 11:50:53.269244       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1219 11:50:53.269252       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I1219 11:50:53.269287       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1219 11:50:53.369966       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1219 11:50:53.369967       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1219 11:50:53.370088       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"


==> kubelet <==
Dec 19 11:53:15 minikube kubelet[1358]: E1219 11:53:15.212266    1358 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Dec 19 11:53:15 minikube kubelet[1358]: E1219 11:53:15.212378    1358 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert podName:980d8aca-bb95-400b-a648-9b5e9e20a2d3 nodeName:}" failed. No retries permitted until 2025-12-19 11:55:17.212360479 +0000 UTC m=+267.401226930 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-ch76m" (UID: "980d8aca-bb95-400b-a648-9b5e9e20a2d3") : secret "ingress-nginx-admission" not found
Dec 19 11:53:26 minikube kubelet[1358]: E1219 11:53:26.079032    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:53:27 minikube kubelet[1358]: E1219 11:53:27.092695    1358 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:53:27 minikube kubelet[1358]: E1219 11:53:27.092735    1358 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:53:27 minikube kubelet[1358]: E1219 11:53:27.092805    1358 kuberuntime_manager.go:1449] "Unhandled Error" err="container create start failed in pod ingress-nginx-admission-create-rmwd6_ingress-nginx(be1d9435-ae41-4d4a-b5fa-f1529062b9b8): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" logger="UnhandledError"
Dec 19 11:53:27 minikube kubelet[1358]: E1219 11:53:27.092841    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:53:38 minikube kubelet[1358]: E1219 11:53:38.079237    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:53:49 minikube kubelet[1358]: E1219 11:53:49.079423    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:53:53 minikube kubelet[1358]: E1219 11:53:53.096532    1358 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:53:53 minikube kubelet[1358]: E1219 11:53:53.096569    1358 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:53:53 minikube kubelet[1358]: E1219 11:53:53.096635    1358 kuberuntime_manager.go:1449] "Unhandled Error" err="container patch start failed in pod ingress-nginx-admission-patch-fqhwk_ingress-nginx(1294573e-02f9-4063-94b2-185771a65203): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" logger="UnhandledError"
Dec 19 11:53:53 minikube kubelet[1358]: E1219 11:53:53.096658    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:54:03 minikube kubelet[1358]: E1219 11:54:03.078841    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:54:08 minikube kubelet[1358]: E1219 11:54:08.079762    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:54:15 minikube kubelet[1358]: E1219 11:54:15.079455    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:54:19 minikube kubelet[1358]: E1219 11:54:19.079163    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:54:28 minikube kubelet[1358]: E1219 11:54:28.079059    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:54:33 minikube kubelet[1358]: E1219 11:54:33.079722    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:54:42 minikube kubelet[1358]: E1219 11:54:42.083261    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:54:45 minikube kubelet[1358]: E1219 11:54:45.078889    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:54:56 minikube kubelet[1358]: E1219 11:54:56.079601    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:55:08 minikube kubelet[1358]: E1219 11:55:08.092591    1358 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:55:08 minikube kubelet[1358]: E1219 11:55:08.092631    1358 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:55:08 minikube kubelet[1358]: E1219 11:55:08.092711    1358 kuberuntime_manager.go:1449] "Unhandled Error" err="container create start failed in pod ingress-nginx-admission-create-rmwd6_ingress-nginx(be1d9435-ae41-4d4a-b5fa-f1529062b9b8): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" logger="UnhandledError"
Dec 19 11:55:08 minikube kubelet[1358]: E1219 11:55:08.092742    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:55:09 minikube kubelet[1358]: E1219 11:55:09.078946    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:55:17 minikube kubelet[1358]: E1219 11:55:17.270942    1358 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Dec 19 11:55:17 minikube kubelet[1358]: E1219 11:55:17.271008    1358 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert podName:980d8aca-bb95-400b-a648-9b5e9e20a2d3 nodeName:}" failed. No retries permitted until 2025-12-19 11:57:19.270990215 +0000 UTC m=+389.459856669 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-ch76m" (UID: "980d8aca-bb95-400b-a648-9b5e9e20a2d3") : secret "ingress-nginx-admission" not found
Dec 19 11:55:20 minikube kubelet[1358]: E1219 11:55:20.079962    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:55:26 minikube kubelet[1358]: E1219 11:55:26.078656    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-ch76m" podUID="980d8aca-bb95-400b-a648-9b5e9e20a2d3"
Dec 19 11:55:35 minikube kubelet[1358]: E1219 11:55:35.083103    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:55:37 minikube kubelet[1358]: E1219 11:55:37.118666    1358 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:55:37 minikube kubelet[1358]: E1219 11:55:37.118703    1358 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 19 11:55:37 minikube kubelet[1358]: E1219 11:55:37.118766    1358 kuberuntime_manager.go:1449] "Unhandled Error" err="container patch start failed in pod ingress-nginx-admission-patch-fqhwk_ingress-nginx(1294573e-02f9-4063-94b2-185771a65203): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": context deadline exceeded" logger="UnhandledError"
Dec 19 11:55:37 minikube kubelet[1358]: E1219 11:55:37.118790    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:55:47 minikube kubelet[1358]: E1219 11:55:47.078810    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:55:52 minikube kubelet[1358]: E1219 11:55:52.079921    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:55:59 minikube kubelet[1358]: E1219 11:55:59.079109    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:56:05 minikube kubelet[1358]: E1219 11:56:05.078849    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:56:13 minikube kubelet[1358]: E1219 11:56:13.079521    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:56:20 minikube kubelet[1358]: E1219 11:56:20.079703    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:56:24 minikube kubelet[1358]: E1219 11:56:24.079583    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:56:32 minikube kubelet[1358]: E1219 11:56:32.078952    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:56:35 minikube kubelet[1358]: E1219 11:56:35.079505    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:56:45 minikube kubelet[1358]: E1219 11:56:45.079074    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:56:47 minikube kubelet[1358]: E1219 11:56:47.079688    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:56:59 minikube kubelet[1358]: E1219 11:56:59.079010    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:57:00 minikube kubelet[1358]: E1219 11:57:00.079562    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:57:12 minikube kubelet[1358]: E1219 11:57:12.079497    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:57:14 minikube kubelet[1358]: E1219 11:57:14.079102    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:57:19 minikube kubelet[1358]: E1219 11:57:19.279704    1358 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Dec 19 11:57:19 minikube kubelet[1358]: E1219 11:57:19.279807    1358 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert podName:980d8aca-bb95-400b-a648-9b5e9e20a2d3 nodeName:}" failed. No retries permitted until 2025-12-19 11:59:21.279794227 +0000 UTC m=+511.468660678 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/980d8aca-bb95-400b-a648-9b5e9e20a2d3-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-ch76m" (UID: "980d8aca-bb95-400b-a648-9b5e9e20a2d3") : secret "ingress-nginx-admission" not found
Dec 19 11:57:24 minikube kubelet[1358]: E1219 11:57:24.079064    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:57:27 minikube kubelet[1358]: E1219 11:57:27.078586    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:57:35 minikube kubelet[1358]: E1219 11:57:35.079246    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:57:42 minikube kubelet[1358]: E1219 11:57:42.079274    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"
Dec 19 11:57:44 minikube kubelet[1358]: E1219 11:57:44.078805    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-ch76m" podUID="980d8aca-bb95-400b-a648-9b5e9e20a2d3"
Dec 19 11:57:46 minikube kubelet[1358]: E1219 11:57:46.079544    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-create-rmwd6" podUID="be1d9435-ae41-4d4a-b5fa-f1529062b9b8"
Dec 19 11:57:53 minikube kubelet[1358]: E1219 11:57:53.079293    1358 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-admission-patch-fqhwk" podUID="1294573e-02f9-4063-94b2-185771a65203"


==> storage-provisioner [0ef6fde88ba7] <==
W1219 11:56:53.965331       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:53.972195       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:55.974123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:55.981685       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:57.983965       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:57.990650       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:56:59.992782       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:00.000139       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:02.002561       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:02.010191       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:04.012392       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:04.018912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:06.021052       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:06.027411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:08.030291       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:08.037951       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:10.040910       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:10.047698       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:12.049925       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:12.057162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:14.058943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:14.065330       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:16.067471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:16.070406       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:18.072331       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:18.078950       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:20.081940       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:20.088537       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:22.090566       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:22.098822       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:24.101434       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:24.104431       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:26.107203       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:26.113642       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:28.115609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:28.123645       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:30.126256       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:30.133255       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:32.135028       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:32.141375       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:34.144007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:34.150424       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:36.153024       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:36.159441       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:38.161976       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:38.169481       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:40.172213       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:40.178687       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:42.182814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:42.186156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:44.188764       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:44.195219       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:46.197021       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:46.203957       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:48.205913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:48.212117       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:50.214889       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:50.222629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:52.224727       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1219 11:57:52.231049       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [9a2ca6412bf9] <==
I1219 11:50:54.925532       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1219 11:51:24.941964       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

